{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot-Check Regression Algorithms\n",
    "\n",
    "><small><i>from the book \n",
    "\"Machine Learning Mastery With Python: Understand Your Data, Create Accurate Models and Work Projects End-To-End\"\n",
    "by Jason Brownlee, Migrated to Jupyter with additions by Mitch Sanders 2017</i></small>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot-checking is a way of discovering which algorithms perform well on your machine learning\n",
    "problem. You cannot know which algorithms are best suited to your problem beforehand. You\n",
    "must trial a number of methods and focus attention on those that prove themselves the most\n",
    "promising. In this chapter you will discover six machine learning algorithms that you can use\n",
    "when spot-checking your regression problem in Python with scikit-learn. After completing this\n",
    "lesson you will know:\n",
    "\n",
    "1. How to spot-check machine learning algorithms on a regression problem.\n",
    "2. How to spot-check four linear regression algorithms.\n",
    "3. How to spot-check three nonlinear regression algorithms.\n",
    "\n",
    "Letâ€™s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms Overview\n",
    "In this lesson we are going to take a look at seven regression algorithms that you can spot-check\n",
    "on your dataset. Starting with four linear machine learning algorithms:\n",
    "\n",
    "- Linear Regression.\n",
    "- Ridge Regression.\n",
    "- LASSO Linear Regression.\n",
    "- Elastic Net Regression.\n",
    "\n",
    "Then looking at three nonlinear machine learning algorithms:\n",
    "\n",
    "- k-Nearest Neighbors.\n",
    "- Classification and Regression Trees.\n",
    "- Support Vector Machines.\n",
    "\n",
    "Each recipe is demonstrated on the **Boston House Price** dataset. This is a regression\n",
    "problem where all attributes are numeric. A test harness with 10-fold cross-validation is used\n",
    "to demonstrate how to spot-check each machine learning algorithm and mean squared error\n",
    "measures are used to indicate algorithm performance. Note that mean squared error values are\n",
    "inverted (negative). This is a quirk of the **cross val score()** function used that requires all\n",
    "algorithm metrics to be sorted in ascending order (larger value is better). The recipes assume\n",
    "that you know about each machine learning algorithm and how to use them. We will not go\n",
    "into the API or parameterization of each algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Machine Learning Algorithms\n",
    "This section provides examples of how to use four different linear machine learning algorithms\n",
    "for regression in Python with scikit-learn.\n",
    "\n",
    "### Linear Regression\n",
    "Linear regression assumes that the input variables have a Gaussian distribution. It is also\n",
    "assumed that input variables are relevant to the output variable and that they are not highly\n",
    "correlated with each other (a problem called collinearity). You can construct a linear regression\n",
    "model using the LinearRegression class.\n",
    "\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.7052559445\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "filename = '../housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example provides a estimate of mean squared error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Ridge regression is an extension of linear regression where the loss function is modified to\n",
    "minimize the complexity of the model measured as the sum squared value of the coefficient\n",
    "values (also called the L2-norm). You can construct a ridge regression model by using the Ridge\n",
    "class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.0782462093\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "filename = '../housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = Ridge()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Regression\n",
    "\n",
    "The Least Absolute Shrinkage and Selection Operator (or LASSO for short) is a modification\n",
    "of linear regression, like ridge regression, where the loss function is modified to minimize the\n",
    "complexity of the model measured as the sum absolute value of the coefficient values (also called\n",
    "the L1-norm). You can construct a LASSO model by using the Lasso class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.4640845883\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "filename = '../housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = Lasso()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "# Running the example provides an estimate of the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Regression\n",
    "\n",
    "ElasticNet is a form of regularization regression that combines the properties of both Ridge\n",
    "Regression and LASSO regression. It seeks to minimize the complexity of the regression model\n",
    "(magnitude and number of regression coefficients) by penalizing the model using both the\n",
    "L2-norm (sum squared coefficient values) and the L1-norm (sum absolute coefficient values).\n",
    "You can construct an ElasticNet model using the ElasticNet class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31.1645737142\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "filename = '../housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = ElasticNet()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example provides an estimate of the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Machine Learning Algorithms\n",
    "\n",
    "This section provides examples of how to use three different nonlinear machine learning algorithms\n",
    "for regression in Python with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "The k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the\n",
    "training dataset for a new data instance. From the k neighbors, a mean or median output\n",
    "variable is taken as the prediction. Of note is the distance metric used (the metric argument).\n",
    "The Minkowski distance is used by default, which is a generalization of both the Euclidean\n",
    "distance (used when all inputs have the same scale) and Manhattan distance (for when the\n",
    "scales of the input variables differ). You can construct a KNN model for regression using the\n",
    "KNeighborsRegressor class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-107.28683898\n"
     ]
    }
   ],
   "source": [
    "# KNN Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "filename = '../housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = KNeighborsRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example provides an estimate of the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression Trees\n",
    "\n",
    "Decision trees or the Classification and Regression Trees (CART as they are known) use the training\n",
    "data to select the best points to split the data in order to minimize a cost metric. The default\n",
    "cost metric for regression decision trees is the mean squared error, specified in the criterion\n",
    "parameter. You can create a CART model for regression using the DecisionTreeRegressor\n",
    "class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39.6383321569\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "filename = '../housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = DecisionTreeRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example provides an estimate of the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "Support Vector Machines (SVM) were developed for binary classification. The technique has\n",
    "been extended for the prediction real-valued problems called Support Vector Regression (SVR).\n",
    "Like the classification example, SVR is built upon the LIBSVM library. You can create an SVM\n",
    "model for regression using the SVR class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-91.0478243332\n"
     ]
    }
   ],
   "source": [
    "# SVM Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "filename = '../housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = SVR()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example provides an estimate of the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this chapter you discovered how to spot-check machine learning algorithms for regression\n",
    "problems in Python using scikit-learn. Specifically, you learned about four linear machine\n",
    "learning algorithms: Linear Regression, Ridge Regression, LASSO Linear Regression and Elastic\n",
    "Net Regression. You also learned about three nonlinear algorithms: k-Nearest Neighbors,\n",
    "Classification and Regression Trees and Support Vector Machines.\n",
    "### Next\n",
    "Now that you know how to use classification and regression algorithms you need to know how\n",
    "to compare the results of different algorithms to each other. In the next lesson you will discover\n",
    "how to design simple experiments to directly compare machine learning algorithms to each other\n",
    "on your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Boston House Price dataset:\n",
    "Maintained at UCI machine Learning Repository\n",
    "https://archive.ics.uci.edu/ml/datasets/housing\n",
    "\n",
    "Included in scikit-learn datasets module\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
    "\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "1. CRIM: per capita crime rate by town \n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "3. INDUS: proportion of non-retail business acres per town \n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "5. NOX: nitric oxides concentration (parts per 10 million) \n",
    "6. RM: average number of rooms per dwelling \n",
    "7. AGE: proportion of owner-occupied units built prior to 1940 \n",
    "8. DIS: weighted distances to five Boston employment centres \n",
    "9. RAD: index of accessibility to radial highways \n",
    "10. TAX: full-value property-tax rate per 10,000 US Dollars\n",
    "11. PTRATIO: pupil-teacher ratio by town \n",
    "12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "13. LSTAT: % lower status of the population \n",
    "14. MEDV: Median value of owner-occupied homes in 1000's US Dollars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "# demo using Boston Housing data in SciKit-learn\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Pima Indian Dataset \n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
