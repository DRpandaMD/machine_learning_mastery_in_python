{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Performance with Ensembles\n",
    "\n",
    "><small><i>from the book \n",
    "\"Machine Learning Mastery With Python: Understand Your Data, Create Accurate Models and Work Projects End-To-End\"\n",
    "by Jason Brownlee, Migrated to Jupyter with additions by Mitch Sanders 2017</i></small>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembles can give you a boost in accuracy on your dataset. In this chapter you will discover\n",
    "how you can create some of the most powerful types of ensembles in Python using scikit-learn.\n",
    "This lesson will step you through Boosting, Bagging and Majority Voting and show you how you\n",
    "can continue to ratchet up the accuracy of the models on your own datasets. After completing\n",
    "this lesson you will know:\n",
    "\n",
    "1. How to use bagging ensemble methods such as bagged decision trees, random forest and extra trees.\n",
    "2. How to use boosting ensemble methods such as AdaBoost and stochastic gradient boosting.\n",
    "3. How to use voting ensemble methods to combine the predictions from multiple algorithms.\n",
    "\n",
    "Letâ€™s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Models Into Ensemble Predictions\n",
    "\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "- **Bagging.** Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "- **Boosting.** Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.\n",
    "- **Voting.** Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions.\n",
    "\n",
    "This assumes you are generally familiar with machine learning algorithms and ensemble\n",
    "methods and will not go into the details of how the algorithms work or their parameters.\n",
    "The Pima Indians onset of Diabetes dataset is used to demonstrate each algorithm. Each\n",
    "ensemble algorithm is demonstrated using 10-fold cross-validation and the classification accuracy\n",
    "performance metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Algorithms\n",
    "\n",
    "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset\n",
    "(with replacement) and training a model for each sample. The final output prediction is averaged\n",
    "across the predictions of all of the sub-models. The three bagging models covered in this section\n",
    "are as follows:\n",
    "\n",
    "- **Bagged Decision Trees.**\n",
    "- **Random Forest.**\n",
    "- **Extra Trees.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagged Decision Trees\n",
    "\n",
    "Bagging performs best with algorithms that have high variance. A popular example are\n",
    "decision trees, often constructed without pruning. In the example below is an example\n",
    "of using the BaggingClassifier with the Classification and Regression Trees algorithm\n",
    "(DecisionTreeClassifier). A total of 100 trees are created.\n",
    "\n",
    "Running the example, we get a robust estimate of model accuracy.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random Forests is an extension of bagged decision trees. Samples of the training dataset are\n",
    "taken with replacement, but the **trees are constructed in a way that reduces the correlation\n",
    "between individual classifiers**. Specifically, rather than greedily choosing the best split point in\n",
    "the construction of each tree, only **a random subset of features are considered for each split**. You\n",
    "can construct a Random Forest model for classification using the RandomForestClassifier\n",
    "class. The example below demonstrates using Random Forest for classification with 100 trees\n",
    "and split points chosen from a random selection of 3 features.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Running the example provides a mean estimate of classification accuracy.\n",
    "\n",
    "*(a good question to answer then as a test is: What's Random in a Random Forest model? Ans: the features themselves, that create is split point (branch) :))*\n",
    "\n",
    "Notes: So to explain further what \"Random Forest\" is, you must explain in this order these concepts:\n",
    "1. A decision tree model?\n",
    "2. An ensemble of decision trees? (aka Whats boot-strapped aggregation? [bagg'ed])\n",
    "3. The randomness in a ensemble of trees?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "# notice RandomForestClassifier itself hides under the hood much of what happens\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees\n",
    "Extra Trees are another modification of bagging where **random trees** are constructed from\n",
    "samples of the training dataset. You can construct an Extra Trees model for classification using\n",
    "the ExtraTreesClassifier class. The example below provides a demonstration of extra trees\n",
    "with the number of trees set to 100 and splits chosen from 7 random features.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "\n",
    "Running the example provides a mean estimate of classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extra Trees Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any thoughts on why would \"Random Trees\" perhaps perform worse than \"Random Forest\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Algorithms\n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes\n",
    "of the models before them in the sequence. Once created, <u>the models make predictions which\n",
    "may be weighted by their demonstrated accuracy and the results are combined</u> to create a final\n",
    "output prediction. The two most common boosting ensemble machine learning algorithms are:\n",
    "\n",
    "- **AdaBoost.**\n",
    "- **Stochastic Gradient Boosting.**\n",
    "\n",
    "\n",
    "### AdaBoost\n",
    "AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by\n",
    "weighting instances in the dataset by how easy or difficult they are to classify, allowing the\n",
    "algorithm to pay less attention to them in the construction of subsequent models. You can\n",
    "construct an AdaBoost model for classification using the AdaBoostClassifier class. The\n",
    "example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost\n",
    "algorithm.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AdaBoost Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_trees = 30\n",
    "seed=7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "\n",
    "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most\n",
    "sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of\n",
    "the best techniques available for improving performance via ensembles. You can construct a\n",
    "Gradient Boosting model for classification using the GradientBoostingClassifier class. The example below demonstrates Stochastic Gradient Boosting for classification with 100 trees.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "Example of Stochastic Gradient Boosting Ensemble Algorithm.\n",
    "\n",
    "Running the example provides a mean estimate of classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Ensemble\n",
    "\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning\n",
    "algorithms. It works by first creating two or more standalone models from your training dataset.\n",
    "A Voting Classifier can then be used to wrap your models and average the predictions of the\n",
    "sub-models when asked to make predictions for new data. The predictions of the sub-models can\n",
    "be weighted, but specifying the weights for classifiers manually or even heuristically is difficult.\n",
    "More advanced methods can learn how to best weight the predictions from sub-models, but this\n",
    "is called stacking (stacked aggregation) and is currently not provided in scikit-learn.\n",
    "\n",
    "\n",
    "You can create a voting ensemble model for classification using the VotingClassifier\n",
    "class. The code below provides an example of combining the predictions of logistic regression,\n",
    "classification and regression trees and support vector machines together for a classification\n",
    "problem\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729118250171\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter you discovered ensemble machine learning algorithms for improving the performance\n",
    "of models on your problems. You learned about:\n",
    "\n",
    "- Bagging Ensembles including Bagged Decision Trees, Random Forest and Extra Trees.\n",
    "- Boosting Ensembles including AdaBoost and Stochastic Gradient Boosting.\n",
    "- Voting Ensembles for averaging the predictions for any arbitrary models.\n",
    "\n",
    "### Next\n",
    "\n",
    "In the next section you will discover another technique that you can use to improve the\n",
    "performance of algorithms on your dataset called **algorithm tuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Pima Indian Dataset \n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
